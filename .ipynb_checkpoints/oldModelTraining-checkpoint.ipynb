{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define a few functions for reuseability. \n",
    "\n",
    "We will use K-fold cross validation both in basic model testing and parameter searching, see documentation:\n",
    " - [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)\n",
    " - [cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
    " - [GridSearchCV](http://scikit-learn.org/dev/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_x0, train_y = split_xy(makeStr(featureFix(scalePrice(impData0))))\n",
    "train_x1, _ = split_xy(makeStr(featureFix(scalePrice(impData1))))\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle = True).get_n_splits(train_x0)\n",
    "\n",
    "def rmse(model,x,y = train_y):\n",
    "    rmse= np.sqrt(-cross_val_score(model, x.values, y.values, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pathos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8796e96c3522>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Note: I'm far more familiar with parallelizing in C++ and Scala, this code is likely not a canonical implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpathos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProcessingPool\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhelper_itModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelConstructor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pathos'"
     ]
    }
   ],
   "source": [
    "# Note: I'm far more familiar with parallelizing in C++ and Scala, this code is likely not a canonical implementation\n",
    "\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "\n",
    "def helper_itModel(modelConstructor, data0, data1):\n",
    "    model0 = modelConstructor\n",
    "    model1 = modelConstructor\n",
    "    cv0 = rmse(model0,data0)\n",
    "    cv1 = rmse(model1,data1)\n",
    "    return (np.mean(cv0),np.mean(cv1),np.std(cv0),np.std(cv1))\n",
    "\n",
    "#### This is where the number of threads is used  ####\n",
    "def itModel(n, modelConstructor, data0, data1, threads = nthreads):\n",
    "    def tempFunc(modelConstructor):\n",
    "        return helper_itModel(modelConstructor,data0,data1)\n",
    "    procList = []\n",
    "    for i in range(n):\n",
    "        procList.append(modelConstructor)\n",
    "    with Pool(processes=threads) as pool:\n",
    "        multiOut = pool.map(tempFunc,procList)\n",
    "    means = np.zeros(shape=(2,n))\n",
    "    stds = np.zeros(shape=(2,n))\n",
    "    for i in range(n):\n",
    "        means[0,i] = multiOut[i][0]\n",
    "        means[1,i] = multiOut[i][1]\n",
    "        stds[0,i] = multiOut[i][2]\n",
    "        stds[1,i] = multiOut[i][3]\n",
    "    return(means,stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def ttest_model(n, modelConstructor, data0, data1, threads = nthreads):\n",
    "    means, _ = itModel(n, modelConstructor, data0, data1, nthreads)\n",
    "    ttest = ttest_ind(means[0,:],means[1,:])\n",
    "    diff = np.mean(means[0,:]) - np.mean(means[1,:])\n",
    "    return (ttest,diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Imputation Comparison\n",
    "\n",
    "As promised, here is the comparison between imputing zero on LotFrontage (index zero) and the more complicated method of imputing on the modal value in each neighborhood (index 1.) We'll also look into the efficacy of flagging where values were imputed in the data set for decision trees, random forests, and gradient boosted trees using XG Boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1_ttest, tree1_diff = ttest_model(50,DecisionTreeRegressor(), train_x0, train_x1)\n",
    "\n",
    "print(tree1_diff)\n",
    "print(tree1_ttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_ttest, rf_diff   = ttest_model(50,RandomForestRegressor(), train_x0, train_x1)\n",
    "\n",
    "print(rf_diff)\n",
    "print(rf_ttest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, imputing zero on the LotFrontage feature works slightly better (statistically significant) for a decision tree regressor but has an insignificant effect  for a random forest regressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputeVals_noflag_0(in_df):\n",
    "    df = in_df.copy()\n",
    "    for i in fillNone:\n",
    "        df[i] = df[i].fillna(\"None\")\n",
    "    for i in fillZero:\n",
    "        #df[\"null_%s\" % (i)] = df[i].isnull()                           # mark which zeros are imputed\n",
    "        df[i] = df[i].fillna(0)\n",
    "    df.Electrical = df.Electrical.fillna(\"SBrkr\")\n",
    "    df.Functional = df.Functional.fillna(\"Typ\")                        # Documentation instructs to assume \"typical\" unless otherwise noted\n",
    "    df.CentralAir = df.CentralAir.fillna(\"Y\")\n",
    "    #df[\"null_LotFrontage\"] = df.LotFrontage.isnull()\n",
    "    df.LotFrontage = df.LotFrontage.fillna(0)                             \n",
    "    df.MSZoning = df.MSZoning.fillna(df.Neighborhood.map(zoning))\n",
    "    df.Utilities = df.Utilities.fillna(df.Neighborhood.map(utilities))\n",
    "    df.KitchenQual = df.KitchenQual.fillna(\"Po\")                      #one house missing kitchen data\n",
    "    df.SaleType = df.SaleType.fillna(\"Oth\")                           # only one missing value, fill the already defined \"other\"\n",
    "    df.Exterior1st = df.Exterior1st.fillna(\"Other\")\n",
    "    df.Exterior2nd = df.Exterior2nd.fillna(\"Other\")                  # the same house is responsible for the missing exterior 1st and 2nd, other is predefined\n",
    "    df = df.drop(columns=[\"Id\"])\n",
    "    return(df)\n",
    "\n",
    "def imputeVals_noflag_1(in_df):\n",
    "    df = in_df.copy()\n",
    "    for i in fillNone:\n",
    "        df[i] = df[i].fillna(\"None\")\n",
    "    for i in fillZero:\n",
    "        #df[\"null_%s\" % (i)] = df[i].isnull()\n",
    "        df[i] = df[i].fillna(0)\n",
    "    df.Electrical = df.Electrical.fillna(\"SBrkr\")\n",
    "    df.Functional = df.Functional.fillna(\"Typ\")                        \n",
    "    df.CentralAir = df.CentralAir.fillna(\"Y\")\n",
    "    df.LotFrontage = df.LotFrontage.fillna(df.Neighborhood.map(frontage))            # This is the only line different in these two functions, maybe a more elegant solution is possible               \n",
    "    df.MSZoning = df.MSZoning.fillna(df.Neighborhood.map(zoning))\n",
    "    df.Utilities = df.Utilities.fillna(df.Neighborhood.map(utilities))\n",
    "    df.KitchenQual = df.KitchenQual.fillna(\"Po\")                     \n",
    "    df.SaleType = df.SaleType.fillna(\"Oth\")                           \n",
    "    df.Exterior1st = df.Exterior1st.fillna(\"Other\")\n",
    "    df.Exterior2nd = df.Exterior2nd.fillna(\"Other\")                 \n",
    "    df = df.drop(columns=[\"Id\"])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_noflag_0, _ = split_xy(makeStr(featureFix(scalePrice(imputeVals_noflag_0(trainData)))))\n",
    "train_x_noflag_1, _ = split_xy(makeStr(featureFix(scalePrice(imputeVals_noflag_1(trainData)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms the suspicion that adding those flags does little to improve the quality of the base decision tree, despite being statistically significant. Maybe we would use them if we were playing for inches.  \n",
    "How about with a random forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rf_0_flags_test, rf_0_flags_diff = ttest_model(n = 50, modelConstructor = RandomForestRegressor(), data0 = train_x0, data1 = train_x_noflag_0)\n",
    "print(rf_0_flags_diff)\n",
    "print(rf_0_flags_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flags appear to do nothing for the random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to randomly generate the state for XGBRegressor because it is otherwise defaulted to a value of zero. Otherwise we will produce a set of identical values for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "def nextRand(): \n",
    "    return randint(0,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LONG RUN TIME\n",
    "#xgb_noflag_test, xgb_noflag_diff = ttest_model(n = 50, modelConstructor = XGBRegressor(random_state=nextRand()), data0 = train_x_noflag_0, data1 = train_x_noflag_1, threads = nthreads)\n",
    "#print(xgb_noflag_diff)\n",
    "#print(xgb_noflag_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Delta$ = 0.0014741226742583935  \n",
    "Ttest_indResult(statistic=185888033141677.97, pvalue=0.0)  \n",
    "\n",
    "This result is likely due to the the random state seeding of XGBoost however a workaround is still a work in progress. In other languages call by name evaluation of nextRand() would take care of the problem. It's also possible that the pickling used in multithreading reduces to a single value, this would also be inconsequential in languages with native parallelization.\n",
    "\n",
    "It should be noted that the exact same values are produced whether flags are kept or not if the random state is not modulated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Delta$ = 0.0014741226742583935  \n",
    "Ttest_indResult(statistic=185888033141677.97, pvalue=0.0)  \n",
    "\n",
    "This result is likely due to the the random state seeding of XGBoost however a workaround is still a work in progress. In other languages call by name evaluation of nextRand() would take care of the problem. It's also possible that the pickling used in multithreading reduces to a single value, this would also be inconsequential in languages with native parallelization.\n",
    "\n",
    "It should be noted that the exact same values are produced whether flags are kept or not if the random state is not modulated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light GBM performs the same function as XGBoost but it's faster though more sensitive to over fitting. Let's see how it performs on a data set of this size (~1400 items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_noflag_test, lgb_noflag_diff = ttest_model(n = 50, modelConstructor = lgb.LGBMRegressor(objective='regression'), data0 = train_x_noflag_0, data1 = train_x_noflag_1, threads = nthreads)\n",
    "print(lgb_noflag_diff)\n",
    "print(lgb_noflag_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again it seems we are stuck due to random seeds. It should be noted that this is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text block below is for parameter search and has been converted to markdown to avoid being run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "         \"learning_rate\": np.linspace(0.032,0.033,3),\n",
    "         \"max_depth\": [3],\n",
    "         \"n_estimators\": [2000],\n",
    "         \"colsample_bytree\": np.linspace(0.065,0.075,3),\n",
    "         \"gamma\": [0.01],\n",
    "         \"silent\": [1],\n",
    "         \"min_child_weight\": [1],\n",
    "         \"reg_alpha\": np.linspace(0.2,0.4,3),\n",
    "         \"reg_lambda\": np.linspace(0.55,0.65,3),\n",
    "         \"subsample\": [0.5],\n",
    "         \"nthread\": [1]\n",
    "         }\n",
    "\n",
    "xgb0 = XGBRegressor()\n",
    "\n",
    "###   Potential LONG RUN TIME    ### \n",
    "xgb_grid = GridSearchCV(xgb0,\n",
    "                        xgb_params,\n",
    "                        cv = 3,\n",
    "                        scoring = \"neg_mean_squared_error\",\n",
    "                        n_jobs = -1,\n",
    "                        verbose=50)\n",
    "\n",
    "xgb_grid.fit(train_x_noflag_1,train_y)\n",
    "\n",
    "print(np.mean(rmse(xgb.XGBRegressor(**xgb_grid.best_params_),train_x_noflag_1,train_y)))\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lgbm_train = lgb.Dataset(train_x_noflag_1, label = train_y)\n",
    "\n",
    "lgbm_params = {'objective' : ['regression'],\n",
    "               'metric': ['mse'],\n",
    "               'num_leaves' : [5,8,12],\n",
    "               'learning_rate' : np.linspace(0.03,0.035,3),\n",
    "               'lambda': np.linspace(0.01,0.03,3),\n",
    "               'max_bin': [100],\n",
    "               'bagging_fraction' : [0.01],\n",
    "               'feature_fraction' : [0.173],\n",
    "               'num_rounds' : [500],\n",
    "               'n_estimators': [750],\n",
    "               'sub_feature' : [0.5],\n",
    "               'verbose': [0]\n",
    "              }\n",
    "\n",
    "lgbm_test = lgb.LGBMRegressor()\n",
    "\n",
    "lgbm_grid =  GridSearchCV(lgbm_test,\n",
    "                        lgbm_params,\n",
    "                        scoring = \"neg_mean_squared_error\",\n",
    "                        cv = 5,\n",
    "                        n_jobs = -1,\n",
    "                        verbose=50)\n",
    "\n",
    "lgbm_grid.fit(train_x_noflag_1,train_y)\n",
    "\n",
    "print(np.mean(rmse(lgb.LGBMRegressor(**lgbm_grid.best_params_),train_x_noflag_1,train_y)))\n",
    "print(lgbm_grid.best_score_)\n",
    "print(lgbm_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLen = trainData.shape[0]\n",
    "\n",
    "train_x, train_y = split_xy(featureFix(scalePrice(imputeVals_noflag_1(trainData))))\n",
    "test_x = featureFix(imputeVals_noflag_1(testData))\n",
    "\n",
    "withDummy = makeStr(pd.concat([train_x,test_x]))\n",
    "\n",
    "train_x = withDummy.iloc[0:trainLen]\n",
    "test_x = withDummy.iloc[trainLen:]\n",
    "\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(colsample_bytree = 0.075, gamma = 0.01, learning_rate = 0.0325, max_depth = 3,\n",
    "                  min_child_weight = 1, n_estimators = 2000, nthread = nthreads, reg_alpha = 0.2, reg_lambda = 0.65,\n",
    "                  silent = 1, subsample = 0.5)\n",
    "\n",
    "xgb.fit(train_x,train_y)\n",
    "\n",
    "xgb_preds = xgb.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rmse(xgb,train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {'bagging_fraction': 0.1, 'feature_fraction': 0.1, 'lambda': 0.1, 'learning_rate': 0.0325, 'max_bin': 60, 'metric': 'mse', 'n_estimators': 750, 'num_leaves': 8, 'objective': 'regression', 'sub_feature': 0.5, 'verbose': 0}\n",
    "lgbm = lgb.LGBMRegressor(**lgbm_params)\n",
    "lgbm.fit(train_x,train_y)\n",
    "lgbm_preds = lgbm.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {'bagging_fraction': 0.1, 'feature_fraction': 0.1, 'lambda': 0.1, 'learning_rate': 0.0325, 'max_bin': 60, 'metric': 'mse', 'n_estimators': 750, 'num_leaves': 8, 'objective': 'regression', 'sub_feature': 0.5, 'verbose': 0}\n",
    "lgbm2 = lgb.LGBMRegressor(**lgbm_params)\n",
    "np.mean(rmse(lgbm2,train_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_frame = pd.DataFrame()\n",
    "submit_frame['Id'] = testID\n",
    "submit_frame['SalePrice'] = invPrice(xgb_preds)\n",
    "submit_frame.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards the future!\n",
    "\n",
    "This performance with a single model is pretty good, but we can do better. To climb the leader boards, it will be necessary to implement an ensemble model.   \n",
    "As a point of good practice, we should also have withheld a train-test split to test our models on in addition to using cross validation. To some extent this is done when you submit for ranking but it can't be done rapidly or in a manner that is conducive to good study of the model.\n",
    "\n",
    "For now, this will suffice until I feel like looking at this data set again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
