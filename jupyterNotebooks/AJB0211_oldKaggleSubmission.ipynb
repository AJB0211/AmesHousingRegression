{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6624b7a221991f707159481c16ba7860fa3819a6"
   },
   "source": [
    "Documentation for the data can be found [here](http://ww2.amstat.org/publications/jse/v19n3/Decock/DataDocumentation.txt). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6eed3aaee0fbc1531b2be8db846af64933d62aa8"
   },
   "source": [
    "I take a lot of inspiration from:\n",
    "* [The Kaggle Learn: Machine Learning lessons by Dan Becker](https://www.kaggle.com/learn/machine-learning)\n",
    "*  [Stacked Regressions by Serigne](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)\n",
    "* [Comprehensive data exploration with Python by  Pedro Marcelino](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b66b76c7ea21187162d83babf220a5939b61dc64"
   },
   "source": [
    "Please pay attention to the first block of code. At the top I define the number of theads to be used as a global variable. I have a Ryzen 7 2700X, which is an 8-core CPU with a base clock speed of 3.7 Ghz. If you try running this notebook, you may run into issues with the number of threads you can run simultaneous or in the run time you experience. This notebook in it's current state still takes about 30 minutes for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "nthreads = 8\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d284d189043cf3be008cd62dfa1f9e7f22f47e01"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "from plotnine import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import norm    # used in plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "401a64265bd0a358c89332ab6ae16f602002cd36"
   },
   "outputs": [],
   "source": [
    "trainData = pd.read_csv(\"../input/train.csv\")\n",
    "testData = pd.read_csv(\"../input/test.csv\")\n",
    "testID = testData.Id\n",
    "trainData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "36f8d6e25df75e65906c016e3f04d49d14cb123f"
   },
   "source": [
    "# Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5554aadcd0b39b2c099ee987ad4447537a55169a"
   },
   "source": [
    "The documentation refers to 5 outliers in this dataset, of which 3 are true outliers. A plot of sale price against ground living area is encouraged. Below, 4 clear outliers, all above 4000 sq ft ground living area are clear. Two of which seem due to large houses while another two are anomalous. We eliminate the anomalous values. The last point may be lost in the test data set or culled for being an outlier, though this cannot be determined without the `SalePrice` values redacted for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4a0a5121a1d8e520f06405da075f0888fbd7d1ff"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=trainData,x=\"GrLivArea\",y=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c1857f1ec71ebe04eac1be3d34944fe58e5bf7b0"
   },
   "outputs": [],
   "source": [
    "outliers = ((trainData.GrLivArea > 4000) & (trainData.SalePrice < 5E5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b797b2f69153e45fa25d266b0b2530e25cbaed5"
   },
   "outputs": [],
   "source": [
    "trainData = trainData[~(outliers)]\n",
    "sns.scatterplot(data=trainData,x=\"GrLivArea\",y=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "21ed0369c2e8d8454672f2f1f92d5e5a47105931"
   },
   "source": [
    "Before continuing we should look at which variables are highly correlated with our SalePrice target. A correlation matrix in the form of a heatmap provides this by reading off color values from the last row of the chart while additionally providing information about correlations between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c3913beff9c0edb89ffd9a82b92865e0f8b3053"
   },
   "outputs": [],
   "source": [
    "f, ax = plt.pyplot.subplots(figsize=(12, 9))\n",
    "sns.heatmap(trainData.corr(),square=True,cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "77acf5150d1bf4e9e0b1c865da79d006d9aee3a1"
   },
   "source": [
    "From this we see the most important variables seem to be `OverallQual`, `TotalBsmtSF`, `1stFlrSF`, `GrLivArea`, `GarageCars`, and `GarageArea`. We should pay extra attention to these later before training the model.\n",
    "Of these, we see that the pairs (`TotalBsmtSF`, `1stFlrSF`) and (`GarageCars`,`GarageArea`) are highly correlated. This is expected, as the basement and first floor are matched to the size of the foundation, and the number of cars that can fit in a garage is going to be dependent on how big the garage is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "30d0188ac13f5461f571122a3ca836a295865e66"
   },
   "outputs": [],
   "source": [
    "impCols = [\"OverallQual\",\"TotalBsmtSF\", \"1stFlrSF\", \"GrLivArea\", \"GarageCars\", \"GarageArea\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e8b335e7893007cd6e1c5866725a62db059e2264"
   },
   "source": [
    "Before checking for missing data, see what the documentation has to say about each column and convert null values to their actual indicator. This will be packed into a function in order to pipeline when making predictions on test data. Here we list the columns transformed:\n",
    "* Alley: no alley\n",
    "*  Bsmt\\*: no basement\n",
    "* FireplaceQu: no fireplace\n",
    "* Garage\\*: no garage\n",
    "    + expect missing values for GarageYrBlt\n",
    "* PoolQC: no pool\n",
    "* Fence: no fence\n",
    "* MiscFeature: no such item as an elevator, tennis court, second garage, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "36f9b052a778d5ba4c48e62742f671ff7a0698d3"
   },
   "outputs": [],
   "source": [
    "def nullfill(in_df):\n",
    "    df = in_df.copy()\n",
    "    df.Alley = df.Alley.fillna(\"None\")\n",
    "    df.BsmtQual = df.BsmtQual.fillna(\"None\")\n",
    "    df.BsmtCond = df.BsmtCond.fillna(\"None\")\n",
    "    df.BsmtExposure = df.BsmtExposure.fillna(\"None\")\n",
    "    df.BsmtFinType1 = df.BsmtFinType1.fillna(\"None\")\n",
    "    df.BsmtFinType2 = df.BsmtFinType2.fillna(\"None\")\n",
    "    df.FireplaceQu = df.FireplaceQu.fillna(\"None\")\n",
    "    df.GarageType = df.GarageType.fillna(\"None\")\n",
    "    df.GarageFinish = df.GarageFinish.fillna(\"None\")\n",
    "    df.GarageQual = df.GarageQual.fillna(\"None\")\n",
    "    df.GarageCond = df.GarageCond.fillna(\"None\")\n",
    "    df.PoolQC = df.PoolQC.fillna(\"None\")\n",
    "    df.Fence = df.Fence.fillna(\"None\")\n",
    "    df.MiscFeature = df.MiscFeature.fillna(\"None\")\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73d562c4af479e4ce242e41974fd83346a4ff032"
   },
   "outputs": [],
   "source": [
    "cleanData = nullfill(trainData)\n",
    "cleanNa = cleanData.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "cleanNa.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "463de34979a313ca49eb038642e980d7baf46838"
   },
   "source": [
    "As expected, the `GarageYrBlt`has null values (as many as were filled from the other garage-related variables).    \n",
    "`MasVnr` is short for Masonry Veneer, which should already have a value \"None\". Still the equality between the missing type and area seems to indicate that this should be fixed anyways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df27d20c9a75aa2382d3ddcbc53eb3160a1d0148"
   },
   "source": [
    "Imputring data for the `Electrical` feature is less intuitive. We could expect the type of electrical system to vary with build date, though updates are possible. Let's see if that's true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "652615efcff560635b6dd9ce4ab20843c1e89e85"
   },
   "outputs": [],
   "source": [
    "( ggplot(trainData, aes(\"YearBuilt\"))\n",
    "   + geom_histogram(bins = 50) \n",
    "   + facet_wrap(\"Electrical\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb574236582d75a6a42c04003a4396ad625662fd"
   },
   "source": [
    "From the above plot it seems safe to impute null values in `Electrical` as \"SBrkr\" seeing as that type is dominant over the time period where null values exist (towards the end of year built data) as well as over the entire range of observed values for year built.\n",
    "\n",
    "(Maybe fix this graph to be proportion rather than raw count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf2ad7406195d054db69b68c64a085a0414ff865"
   },
   "outputs": [],
   "source": [
    "def nullfill2(in_df):\n",
    "    df = nullfill(in_df)\n",
    "    df.MasVnrType = df.MasVnrType.fillna(\"None\")\n",
    "    df.MasVnrArea = df.MasVnrArea.fillna(0)\n",
    "    df.Electrical = df.Electrical.fillna(\"SBrkr\")\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cf4513a21891b8e17dc177547d2c1b7c4b2ea065"
   },
   "outputs": [],
   "source": [
    "cleanData = nullfill2(trainData)\n",
    "cleanNa = cleanData.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "cleanNa.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "17d1456a1188bc1ed6ae720e0db37827363ff92c"
   },
   "source": [
    "A quick check of the test data shows that this cleaning is insufficient, and all cases should be covered. This requires going back and stepping through each class to provide a standard for null inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c958f7093890b9e4321eb239eaf4d8c9dbce37aa"
   },
   "outputs": [],
   "source": [
    "cleanTest = nullfill2(testData)\n",
    "testCleanNA = cleanTest.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "testCleanNA.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b338e9efa4377e6e7006a73ca1c36ebcdea60133"
   },
   "source": [
    "Performing the same analysis we did on the `Electrical` feature, we see that houses in this dataset predominantly have central air. This leads us to impute the `CentralAir` value as \"y\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "722ee25375f5f65220405ee3032bd7e1069d45bb"
   },
   "outputs": [],
   "source": [
    "( ggplot(trainData, aes(\"YearBuilt\"))\n",
    "   + geom_histogram(bins = 50) \n",
    "   + facet_wrap(\"CentralAir\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "27a8e5ae202a550d0b8508c2966be89cf21332d7"
   },
   "source": [
    "We are still left with uncertainty as to what to do with the `GarageYrBlt` and `LotFrontage` features. \n",
    "\n",
    "Concerning the `GarageYrBlt` variable: The plot below shows that the garage is largely built with the house and of those that were not, there is no immediate correlation. Imputing a value of 0 may be dangerous for a continuous variable in a linear regression type model. Additionally, our correlation matrix above shows that this variable may explain some variance in the data so it may not be wise to delete this variable. Consequently, it seems like a good choice to impute values of 0 if we commit to only use decision tree-type models. \n",
    "\n",
    "`LotFrontage` values could mean that the lot doesn't actually have any direct street access. In depth analysis could involve looking at these specific houses to determine what is going on, or seeing correlation with alley access. From there we can impute a value of zero or the modal value for each neighborhood. During the model training phase these two methods will be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e3516d49b29cf88ba59bff4a02dde2ae40d2da34"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=trainData,x=\"YearBuilt\",y=\"GarageYrBlt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c95c380daa0d6510ea54975238c4e4894d4c5ead"
   },
   "outputs": [],
   "source": [
    "(ggplot(trainData,aes(\"LotFrontage\"))\n",
    "  + geom_histogram(bins=30)\n",
    "  + facet_wrap(\"Neighborhood\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f34f2746061c2ca4b34d00ce809f3688e5fe4ba"
   },
   "outputs": [],
   "source": [
    "frontageNull = trainData.LotFrontage.isnull().groupby(trainData[\"Neighborhood\"]).sum()\n",
    "frontageNull.sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b0aee30507ea8394aaa878de3fad8485c06daa1"
   },
   "outputs": [],
   "source": [
    "totalByNeighborhood = trainData.groupby(\"Neighborhood\").count().Id\n",
    "\n",
    "totalByNeighborhood.sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0a1c9e11eb8ee357cd311de30a6b700ad6036ecd"
   },
   "outputs": [],
   "source": [
    "fig1 = sns.scatterplot(x=totalByNeighborhood,y=frontageNull)\n",
    "fig1.set(xlabel=\"Total Observations\",ylabel=\"Number missing\")\n",
    "fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1c6161d7a8245d42cb42513602279a8d312cb930"
   },
   "source": [
    "For the MSZoning and Utilities features we impute on the modal value for each neighborhood, creating a dictionary. We create an additional dictionary to compare our different `LotFrontage` imputations as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f45a1026937c182ded9d7c5326c67ed500711008"
   },
   "outputs": [],
   "source": [
    "zoning = trainData.groupby(\"Neighborhood\").MSZoning.apply(lambda x: x.value_counts().sort_values().index[0]).to_dict()\n",
    "utilities = trainData.groupby(\"Neighborhood\").Utilities.apply(lambda x: x.value_counts().sort_values().index[0]).to_dict()\n",
    "frontage = trainData.groupby(\"Neighborhood\").LotFrontage.apply(lambda x: x.value_counts().sort_values().index[0]).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "131b276b73a078d371693c7a14cb5f2c9c29002e"
   },
   "outputs": [],
   "source": [
    "# values that null is filled with \"None\"\n",
    "fillNone = [\"Alley\",\"BsmtQual\",\"BsmtCond\",\"MasVnrType\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\"FireplaceQu\",\"GarageType\",\"GarageFinish\",\"GarageQual\",\n",
    "           \"GarageCond\",\"PoolQC\",\"Fence\",\"MiscFeature\",\"MasVnrType\"]\n",
    "\n",
    "# For categorical data, it is \n",
    "fillZero = [\"BsmtFinSF1\",\"BsmtFinSF2\",\"BsmtUnfSF\",\"TotalBsmtSF\",\"BsmtFullBath\",\"BsmtHalfBath\",\"FullBath\",\"HalfBath\",\"BedroomAbvGr\",\"2ndFlrSF\",\"GrLivArea\",\n",
    "           \"MasVnrArea\",\"GarageArea\",\"GarageCars\", \"GarageYrBlt\"]\n",
    "\n",
    "def imputeVals0(in_df):\n",
    "    df = in_df.copy()\n",
    "    for i in fillNone:\n",
    "        df[i] = df[i].fillna(\"None\")\n",
    "    for i in fillZero:\n",
    "        df[\"null_%s\" % (i)] = df[i].isnull()                           # mark which zeros are imputed\n",
    "        df[i] = df[i].fillna(0)\n",
    "    df.Electrical = df.Electrical.fillna(\"SBrkr\")\n",
    "    df.Functional = df.Functional.fillna(\"Typ\")                        # Documentation instructs to assume \"typical\" unless otherwise noted\n",
    "    df.CentralAir = df.CentralAir.fillna(\"Y\")\n",
    "    df[\"null_LotFrontage\"] = df.LotFrontage.isnull()\n",
    "    df.LotFrontage = df.LotFrontage.fillna(0)                             \n",
    "    df.MSZoning = df.MSZoning.fillna(df.Neighborhood.map(zoning))\n",
    "    df.Utilities = df.Utilities.fillna(df.Neighborhood.map(utilities))\n",
    "    df.KitchenQual = df.KitchenQual.fillna(\"Po\")                      #one house missing kitchen data\n",
    "    df.SaleType = df.SaleType.fillna(\"Oth\")                           # only one missing value, fill the already defined \"other\"\n",
    "    df.Exterior1st = df.Exterior1st.fillna(\"Other\")\n",
    "    df.Exterior2nd = df.Exterior2nd.fillna(\"Other\")                  # the same house is responsible for the missing exterior 1st and 2nd, other is predefined\n",
    "    df = df.drop(columns=[\"Id\"])\n",
    "    return(df)\n",
    "\n",
    "def imputeVals1(in_df):\n",
    "    df = in_df.copy()\n",
    "    for i in fillNone:\n",
    "        df[i] = df[i].fillna(\"None\")\n",
    "    for i in fillZero:\n",
    "        df[\"null_%s\" % (i)] = df[i].isnull()\n",
    "        df[i] = df[i].fillna(0)\n",
    "    df.Electrical = df.Electrical.fillna(\"SBrkr\")\n",
    "    df.Functional = df.Functional.fillna(\"Typ\")                        \n",
    "    df.CentralAir = df.CentralAir.fillna(\"Y\")\n",
    "    df.LotFrontage = df.LotFrontage.fillna(df.Neighborhood.map(frontage))            # This is the only line different in these two functions, maybe a more elegant solution is possible               \n",
    "    df.MSZoning = df.MSZoning.fillna(df.Neighborhood.map(zoning))\n",
    "    df.Utilities = df.Utilities.fillna(df.Neighborhood.map(utilities))\n",
    "    df.KitchenQual = df.KitchenQual.fillna(\"Po\")                     \n",
    "    df.SaleType = df.SaleType.fillna(\"Oth\")                           \n",
    "    df.Exterior1st = df.Exterior1st.fillna(\"Other\")\n",
    "    df.Exterior2nd = df.Exterior2nd.fillna(\"Other\")                 \n",
    "    df = df.drop(columns=[\"Id\"])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "03582788124a2b907c735959e1507553123f43a5"
   },
   "outputs": [],
   "source": [
    "impData0 = imputeVals0(trainData)\n",
    "impData1 = imputeVals1(trainData)\n",
    "\n",
    "print(\"There are %d nulls left in the training data\" % impData1.isnull().sum().sum())             # Check if both axes can be summed in one function call\n",
    "print(\"There are %d nulls left in the testing data\" % imputeVals0(testData).isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3d13b39746e6efb1e4bcee3d94b276891fefcec7"
   },
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Here we go through the necessary steps of checking the assumptions of most models as well as transformations to optimize model performance. It is worth noting that on a decision tree-type model, renormalization will have little effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7cbfe9a9b2640fcd67dcdd9415cbe9c3585618ef"
   },
   "source": [
    "The plot below shows that the `SalePrice` target variable deviates greatly from a normal distribution. We can attribute this partially to the lower bound of 0 and no upper bound to the value of a home, causing a positive skew.\n",
    "\n",
    "It is worth noting that the kurtosis produced by the default kurtosis function is the excess kurtosis where the normal distribution produces a value zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "969467380768d2ad421d72ea51003210199505ee"
   },
   "outputs": [],
   "source": [
    "sns.distplot(trainData[\"SalePrice\"], fit=norm)\n",
    "print(\"Skew: %.3f\" % (trainData.SalePrice.skew()))\n",
    "print(\"Kurtosis: %.3f\" % (trainData.SalePrice.kurtosis()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e337ed057305be2a953f3f4142eab0491ff3c7d1"
   },
   "source": [
    "The scipy function `normaltest` uses these metrics to produce a statistic that is the sum of the squared skew and kurtosis as well as a p-value for a 2-sided $\\chi^2$ test of normality. Below, the p-value indicates an unfathomably small probability that this sample was drawn from a normal distribution. To improve this, a bit of engineering is in order. A $log$ transformation tempers positive skewness while normalization often improves model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d3a779600e3e29664063aaa877739665b1bc1cdd"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import normaltest\n",
    "\n",
    "normaltest(trainData.SalePrice.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9596b26277b7f315f466edfbdbe8d55ab5020e57"
   },
   "outputs": [],
   "source": [
    "normedPrice = np.log(trainData.SalePrice)\n",
    "sp_mean = np.mean(normedPrice)\n",
    "sp_std = np.std(normedPrice)\n",
    "normedPrice = (normedPrice - sp_mean) / sp_std\n",
    "\n",
    "sns.distplot(normedPrice, fit=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d9a92cb1bd70adb50bfc367d922ee612c92bb952"
   },
   "outputs": [],
   "source": [
    "normaltest(normedPrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab411feb632eea2d44550018972cb24b9a1c8ff4"
   },
   "source": [
    "This visually looks much better and the $\\chi^2$ test shows greatly improved results but it still fails test of normality. More engineering may be necessary to improve performance in the future.\n",
    "\n",
    "Recall we earmarked some features as being important predictors of our target feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2f0f4805ace3e527b2bd4bf8d70cc1041d38fc34"
   },
   "outputs": [],
   "source": [
    "impCols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3922e8a83e41f2fca8b6bde6e69ae9cba1d825a6"
   },
   "source": [
    "Good model performance can depend on the distribution of these values so we renormalize these as well before training. The normaltest performed for the target variable shows that; of our variables of interest; `TotalBsmtSF`, `1stFlrSF`, and `GrLivArea` are all in need of renormalization. Before performing renormalization it's important to note that our imputation procedure for continuous values included imputing zero, particularly for `TotalBsmtSF` where we recognized that houses don't necessarily have basements.  Blindly repeating our previous treatment of `SalePrice` will yield an error because $log(0)$ is undefined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a284ec29f0d53ebddb614c7334aea82f64926f6c"
   },
   "outputs": [],
   "source": [
    "normedCols = impData0[impCols]\n",
    "\n",
    "for col in impCols:\n",
    "    print(\"%s pval: %.3e\" % (col, normaltest(normedCols[col].values)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d4bb81c2688dd3b9ea3aad574c044e7e739df2f4"
   },
   "outputs": [],
   "source": [
    "temp = np.log(normedCols[\"1stFlrSF\"])\n",
    "temp_mean, temp_std = (np.mean(temp),np.std(temp))\n",
    "temp = (temp - temp_mean)/temp_std\n",
    "normedCols[\"1stFlrSF\"] = temp\n",
    "\n",
    "flr1_mean, flr1_std = (temp_mean, temp_std)          # save these values for the transform in the pipeline\n",
    "\n",
    "normaltest(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ec5133f0a5e0ad88bd717a3f3b3efc44cede190d"
   },
   "outputs": [],
   "source": [
    "temp = np.log(normedCols[\"GrLivArea\"])\n",
    "temp_mean, temp_std = (np.mean(temp),np.std(temp))\n",
    "temp = (temp - temp_mean)/temp_std\n",
    "normedCols[\"GrLivArea\"] = temp\n",
    "\n",
    "gflr_mean, gflr_std = (temp_mean, temp_std)    \n",
    "\n",
    "normaltest(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c407faa45bd85356d91d833b4fa58d9c14494d3"
   },
   "outputs": [],
   "source": [
    "normedCols.loc[normedCols.TotalBsmtSF > 0, \"TotalBsmtSF\"] = np.log(normedCols.TotalBsmtSF)      # this must be done with loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0c9106a5fcc6b6346e48d20d0228dab96bf7d59"
   },
   "outputs": [],
   "source": [
    "sns.distplot(normedCols.loc[normedCols.TotalBsmtSF > 0, \"TotalBsmtSF\"], fit=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6813caa1e3074edbec3f6dc5b4d5151958344103"
   },
   "outputs": [],
   "source": [
    "print(\"Skew: %.3f\" % normedCols.loc[normedCols.TotalBsmtSF > 0, \"TotalBsmtSF\"].skew())\n",
    "print(\"Kurtosis: %.3f\" % normedCols.loc[normedCols.TotalBsmtSF > 0, \"TotalBsmtSF\"].kurtosis())\n",
    "normaltest(normedCols.loc[normedCols.TotalBsmtSF > 0, \"TotalBsmtSF\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "388b539117c9627175773fc74de7144f9d5050d7"
   },
   "source": [
    "Despite the poor p-value the distribution of nonzero values is visually improved. We forgo renormalization to avoid overlapping these values with imputed zeros.\n",
    "\n",
    "Now wrap this up in a function for pipelining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "033e663ed9c8405151970eee91b88ba9ced76781"
   },
   "outputs": [],
   "source": [
    "def scalePrice(in_df):\n",
    "    df = in_df.copy()\n",
    "    df.SalePrice = np.log(trainData.SalePrice)\n",
    "    df.SalePrice = (df.SalePrice - sp_mean)/sp_std\n",
    "    return(df)\n",
    "\n",
    "# This function will be necessary to extract predicted values\n",
    "def invPrice(df):\n",
    "    return( np.exp((df*sp_std)+sp_mean))\n",
    "\n",
    "def featureFix(in_df):\n",
    "    df = in_df.copy()\n",
    "    temp = np.log(df[\"1stFlrSF\"])\n",
    "    temp = (temp - flr1_mean)/flr1_std\n",
    "    df[\"1stFlrSF\"] = temp\n",
    "    temp = np.log(df[\"GrLivArea\"])\n",
    "    temp = (temp - gflr_mean)/gflr_std\n",
    "    df[\"GrLivArea\"] = temp\n",
    "    df.loc[df.TotalBsmtSF > 0, \"TotalBsmtSF\"] = np.log(df.TotalBsmtSF)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "faf1566327c74f2cbd89da82b15b26fe7bf202eb"
   },
   "source": [
    "For numerical data that is categorical, make sure that the type is String instead of Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7189908178516301e70b3df661501ffa873476ad"
   },
   "outputs": [],
   "source": [
    "trainData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c540cb3775cd8edc27664f5331c974f46f487253"
   },
   "outputs": [],
   "source": [
    "catCol = [\"MSSubClass\",\"OverallCond\",\"YrSold\",\"MoSold\",\"GarageYrBlt\",\"YearBuilt\",\"YearRemodAdd\"]\n",
    "\n",
    "def makeStr(in_df):\n",
    "    df = in_df.copy()\n",
    "    for col in catCol:\n",
    "        df[col] = df[col].astype(str)\n",
    "    return(pd.get_dummies(df))                          # convert to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "598f6b1c6cbbffd50dfeebf918ee9b508354648b"
   },
   "outputs": [],
   "source": [
    "def split_xy(in_df):\n",
    "    y = in_df.SalePrice\n",
    "    x = in_df.drop(columns=[\"SalePrice\"])\n",
    "    return(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1639fb6118c2d24b89c15c2e2f72913e0e3e205f"
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ae68e9a3214b2fbce95e3d9d2ceca8c4f1269b53"
   },
   "source": [
    "First define a few functions for reuseability. \n",
    "\n",
    "We will use K-fold cross validation both in basic model testing and parameter searching, see documentation:\n",
    " - [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)\n",
    " - [cross_val_score](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
    " - [GridSearchCV](http://scikit-learn.org/dev/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2fa8c63c24c3dd3555b4780b893882d3498f8139"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_x0, train_y = split_xy(makeStr(featureFix(scalePrice(impData0))))\n",
    "train_x1, _ = split_xy(makeStr(featureFix(scalePrice(impData1))))\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle = True).get_n_splits(train_x0)\n",
    "\n",
    "def rmse(model,x,y = train_y):\n",
    "    rmse= np.sqrt(-cross_val_score(model, x.values, y.values, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "918bf8d933df388708f30401b79133f2e9e6e169"
   },
   "outputs": [],
   "source": [
    "# Note: I'm far more familiar with parallelizing in C++ and Scala, this code is likely not a canonical implementation\n",
    "\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "\n",
    "def helper_itModel(modelConstructor, data0, data1):\n",
    "    model0 = modelConstructor\n",
    "    model1 = modelConstructor\n",
    "    cv0 = rmse(model0,data0)\n",
    "    cv1 = rmse(model1,data1)\n",
    "    return (np.mean(cv0),np.mean(cv1),np.std(cv0),np.std(cv1))\n",
    "\n",
    "#### This is where the number of threads is used  ####\n",
    "def itModel(n, modelConstructor, data0, data1, threads = nthreads):\n",
    "    def tempFunc(modelConstructor):\n",
    "        return helper_itModel(modelConstructor,data0,data1)\n",
    "    procList = []\n",
    "    for i in range(n):\n",
    "        procList.append(modelConstructor)\n",
    "    with Pool(processes=threads) as pool:\n",
    "        multiOut = pool.map(tempFunc,procList)\n",
    "    means = np.zeros(shape=(2,n))\n",
    "    stds = np.zeros(shape=(2,n))\n",
    "    for i in range(n):\n",
    "        means[0,i] = multiOut[i][0]\n",
    "        means[1,i] = multiOut[i][1]\n",
    "        stds[0,i] = multiOut[i][2]\n",
    "        stds[1,i] = multiOut[i][3]\n",
    "    return(means,stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b8149739533c511ff603d52362b5061342bcbfb"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def ttest_model(n, modelConstructor, data0, data1, threads = nthreads):\n",
    "    means, _ = itModel(n, modelConstructor, data0, data1, nthreads)\n",
    "    ttest = ttest_ind(means[0,:],means[1,:])\n",
    "    diff = np.mean(means[0,:]) - np.mean(means[1,:])\n",
    "    return (ttest,diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b6b865490a4177ad1818fedc14430fe91c48a039"
   },
   "source": [
    "# Model and Imputation Comparison\n",
    "\n",
    "As promised, here is the comparison between imputing zero on LotFrontage (index zero) and the more complicated method of imputing on the modal value in each neighborhood (index 1.) We'll also look into the efficacy of flagging where values were imputed in the data set for decision trees, random forests, and gradient boosted trees using XG Boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a238ca2c3df371cf3ef4f66296b9bdffd0e4034"
   },
   "source": [
    "tree1_ttest, tree1_diff = ttest_model(50,DecisionTreeRegressor(), train_x0, train_x1)\n",
    "\n",
    "print(tree1_diff)\n",
    "print(tree1_ttest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dbd599b456503373974b5f98ee506fa8f23ecbf4"
   },
   "source": [
    "rf_ttest, rf_diff   = ttest_model(50,RandomForestRegressor(), train_x0, train_x1)\n",
    "\n",
    "print(rf_diff)\n",
    "print(rf_ttest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e9207820b2eaa1dc349ea6cd35d981c145a8ace0"
   },
   "source": [
    "Interestingly, imputing zero on the LotFrontage feature works slightly better (statistically significant) for a decision tree regressor but has an insignificant effect  for a random forest regressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42b00e662adb926bd7623fab6382a9171ee4e662"
   },
   "outputs": [],
   "source": [
    "def imputeVals_noflag_0(in_df):\n",
    "    df = in_df.copy()\n",
    "    for i in fillNone:\n",
    "        df[i] = df[i].fillna(\"None\")\n",
    "    for i in fillZero:\n",
    "        #df[\"null_%s\" % (i)] = df[i].isnull()                           # mark which zeros are imputed\n",
    "        df[i] = df[i].fillna(0)\n",
    "    df.Electrical = df.Electrical.fillna(\"SBrkr\")\n",
    "    df.Functional = df.Functional.fillna(\"Typ\")                        # Documentation instructs to assume \"typical\" unless otherwise noted\n",
    "    df.CentralAir = df.CentralAir.fillna(\"Y\")\n",
    "    #df[\"null_LotFrontage\"] = df.LotFrontage.isnull()\n",
    "    df.LotFrontage = df.LotFrontage.fillna(0)                             \n",
    "    df.MSZoning = df.MSZoning.fillna(df.Neighborhood.map(zoning))\n",
    "    df.Utilities = df.Utilities.fillna(df.Neighborhood.map(utilities))\n",
    "    df.KitchenQual = df.KitchenQual.fillna(\"Po\")                      #one house missing kitchen data\n",
    "    df.SaleType = df.SaleType.fillna(\"Oth\")                           # only one missing value, fill the already defined \"other\"\n",
    "    df.Exterior1st = df.Exterior1st.fillna(\"Other\")\n",
    "    df.Exterior2nd = df.Exterior2nd.fillna(\"Other\")                  # the same house is responsible for the missing exterior 1st and 2nd, other is predefined\n",
    "    df = df.drop(columns=[\"Id\"])\n",
    "    return(df)\n",
    "\n",
    "def imputeVals_noflag_1(in_df):\n",
    "    df = in_df.copy()\n",
    "    for i in fillNone:\n",
    "        df[i] = df[i].fillna(\"None\")\n",
    "    for i in fillZero:\n",
    "        #df[\"null_%s\" % (i)] = df[i].isnull()\n",
    "        df[i] = df[i].fillna(0)\n",
    "    df.Electrical = df.Electrical.fillna(\"SBrkr\")\n",
    "    df.Functional = df.Functional.fillna(\"Typ\")                        \n",
    "    df.CentralAir = df.CentralAir.fillna(\"Y\")\n",
    "    df.LotFrontage = df.LotFrontage.fillna(df.Neighborhood.map(frontage))            # This is the only line different in these two functions, maybe a more elegant solution is possible               \n",
    "    df.MSZoning = df.MSZoning.fillna(df.Neighborhood.map(zoning))\n",
    "    df.Utilities = df.Utilities.fillna(df.Neighborhood.map(utilities))\n",
    "    df.KitchenQual = df.KitchenQual.fillna(\"Po\")                     \n",
    "    df.SaleType = df.SaleType.fillna(\"Oth\")                           \n",
    "    df.Exterior1st = df.Exterior1st.fillna(\"Other\")\n",
    "    df.Exterior2nd = df.Exterior2nd.fillna(\"Other\")                 \n",
    "    df = df.drop(columns=[\"Id\"])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dfa94a866742bf729620e6d95d6d1ad4a65431e1"
   },
   "outputs": [],
   "source": [
    "train_x_noflag_0, _ = split_xy(makeStr(featureFix(scalePrice(imputeVals_noflag_0(trainData)))))\n",
    "train_x_noflag_1, _ = split_xy(makeStr(featureFix(scalePrice(imputeVals_noflag_1(trainData)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b33bef1c20b026ae16dd10df693397a9a9f48177"
   },
   "source": [
    "ttest_model(n = 500, modelConstructor = DecisionTreeRegressor(), data0 = train_x0, data1 = train_x_noflag_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e571bab05026f10e328617e727325be03cb130e8"
   },
   "source": [
    "This confirms the suspicion that adding those flags does little to improve the quality of the base decision tree, despite being statistically significant. Maybe we would use them if we were playing for inches.  \n",
    "How about with a random forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b2f2c5870ee2169dd16b7e66e327be659c8d5c2"
   },
   "source": [
    "rf_0_flags_test, rf_0_flags_diff = ttest_model(n = 50, modelConstructor = RandomForestRegressor(), data0 = train_x0, data1 = train_x_noflag_0)\n",
    "print(rf_0_flags_diff)\n",
    "print(rf_0_flags_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3d9a685be3684cd13cde4a59ffa3f31c656f5e20"
   },
   "source": [
    "Flags appear to do nothing for the random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9729746bce282c7f1fb558dc95ba93c65ce31120"
   },
   "source": [
    "It is necessary to randomly generate the state for XGBRegressor because it is otherwise defaulted to a value of zero. Otherwise we will produce a set of identical values for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae2ba2c440bc8f82d5f54065c0cf31eadaaef46c"
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "def nextRand(): \n",
    "    return randint(0,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a1b1fa59a98165d30cbec9a1776194cb90017689"
   },
   "outputs": [],
   "source": [
    "# LONG RUN TIME\n",
    "#xgb_noflag_test, xgb_noflag_diff = ttest_model(n = 50, modelConstructor = XGBRegressor(random_state=nextRand()), data0 = train_x_noflag_0, data1 = train_x_noflag_1, threads = nthreads)\n",
    "#print(xgb_noflag_diff)\n",
    "#print(xgb_noflag_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0c309b44a64384e4c6b5409de49e4dcda1f6df6e"
   },
   "source": [
    "$\\Delta$ = 0.0014741226742583935  \n",
    "Ttest_indResult(statistic=185888033141677.97, pvalue=0.0)  \n",
    "\n",
    "This result is likely due to the the random state seeding of XGBoost however a workaround is still a work in progress. In other languages call by name evaluation of nextRand() would take care of the problem. It's also possible that the pickling used in multithreading reduces to a single value, this would also be inconsequential in languages with native parallelization.\n",
    "\n",
    "It should be noted that the exact same values are produced whether flags are kept or not if the random state is not modulated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e54b7226e551f9c662414a8e4a1d85cfa808abe"
   },
   "source": [
    "Light GBM performs the same function as XGBoost but it's faster though more sensitive to over fitting. Let's see how it performs on a data set of this size (~1400 items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "94e04c3efb8e2d02688622bd6309e129fa3c868d"
   },
   "source": [
    "lgb_noflag_test, lgb_noflag_diff = ttest_model(n = 50, modelConstructor = lgb.LGBMRegressor(objective='regression'), data0 = train_x_noflag_0, data1 = train_x_noflag_1, threads = nthreads)\n",
    "print(lgb_noflag_diff)\n",
    "print(lgb_noflag_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e92d168a0dcad89c1ef46ea2492b95eb88c182cc"
   },
   "source": [
    "Again it seems we are stuck due to random seeds. It should be noted that this is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b9e62524d97c3a18ec48fddf71429e18a01cf5f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6f289fcb8bbf522df856388a9813e24fb3e235fa"
   },
   "source": [
    "Text block below is for parameter search and has been converted to markdown to avoid being run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "892b2d42eb85c13f4938b41581e5de17ce51839e"
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "         \"learning_rate\": np.linspace(0.032,0.033,3),\n",
    "         \"max_depth\": [3],\n",
    "         \"n_estimators\": [2000],\n",
    "         \"colsample_bytree\": np.linspace(0.065,0.075,3),\n",
    "         \"gamma\": [0.01],\n",
    "         \"silent\": [1],\n",
    "         \"min_child_weight\": [1],\n",
    "         \"reg_alpha\": np.linspace(0.2,0.4,3),\n",
    "         \"reg_lambda\": np.linspace(0.55,0.65,3),\n",
    "         \"subsample\": [0.5],\n",
    "         \"nthread\": [1]\n",
    "         }\n",
    "\n",
    "xgb0 = XGBRegressor()\n",
    "\n",
    "###   Potential LONG RUN TIME    ### \n",
    "xgb_grid = GridSearchCV(xgb0,\n",
    "                        xgb_params,\n",
    "                        cv = 3,\n",
    "                        scoring = \"neg_mean_squared_error\",\n",
    "                        n_jobs = -1,\n",
    "                        verbose=50)\n",
    "\n",
    "xgb_grid.fit(train_x_noflag_1,train_y)\n",
    "\n",
    "print(np.mean(rmse(xgb.XGBRegressor(**xgb_grid.best_params_),train_x_noflag_1,train_y)))\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f780ba34dabcf816cad68b65e367fdd045a17f1"
   },
   "source": [
    "lgbm_train = lgb.Dataset(train_x_noflag_1, label = train_y)\n",
    "\n",
    "lgbm_params = {'objective' : ['regression'],\n",
    "               'metric': ['mse'],\n",
    "               'num_leaves' : [5,8,12],\n",
    "               'learning_rate' : np.linspace(0.03,0.035,3),\n",
    "               'lambda': np.linspace(0.01,0.03,3),\n",
    "               'max_bin': [100],\n",
    "               'bagging_fraction' : [0.01],\n",
    "               'feature_fraction' : [0.173],\n",
    "               'num_rounds' : [500],\n",
    "               'n_estimators': [750],\n",
    "               'sub_feature' : [0.5],\n",
    "               'verbose': [0]\n",
    "              }\n",
    "\n",
    "lgbm_test = lgb.LGBMRegressor()\n",
    "\n",
    "lgbm_grid =  GridSearchCV(lgbm_test,\n",
    "                        lgbm_params,\n",
    "                        scoring = \"neg_mean_squared_error\",\n",
    "                        cv = 5,\n",
    "                        n_jobs = -1,\n",
    "                        verbose=50)\n",
    "\n",
    "lgbm_grid.fit(train_x_noflag_1,train_y)\n",
    "\n",
    "print(np.mean(rmse(lgb.LGBMRegressor(**lgbm_grid.best_params_),train_x_noflag_1,train_y)))\n",
    "print(lgbm_grid.best_score_)\n",
    "print(lgbm_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e2c669306b7f7dda260d7f2dc31cfd8473d200cf"
   },
   "source": [
    "# Prediction and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "698e40b8f573bf7ba2c4dc2e20538b17da9ea6f0"
   },
   "outputs": [],
   "source": [
    "trainLen = trainData.shape[0]\n",
    "\n",
    "train_x, train_y = split_xy(featureFix(scalePrice(imputeVals_noflag_1(trainData))))\n",
    "test_x = featureFix(imputeVals_noflag_1(testData))\n",
    "\n",
    "withDummy = makeStr(pd.concat([train_x,test_x]))\n",
    "\n",
    "train_x = withDummy.iloc[0:trainLen]\n",
    "test_x = withDummy.iloc[trainLen:]\n",
    "\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7e44654fb8aeb116c3455298d48a12783207a4ac"
   },
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(colsample_bytree = 0.075, gamma = 0.01, learning_rate = 0.0325, max_depth = 3,\n",
    "                  min_child_weight = 1, n_estimators = 2000, nthread = nthreads, reg_alpha = 0.2, reg_lambda = 0.65,\n",
    "                  silent = 1, subsample = 0.5)\n",
    "\n",
    "xgb.fit(train_x,train_y)\n",
    "\n",
    "xgb_preds = xgb.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d155327217a9933ee4752646979f1b402f12f614"
   },
   "outputs": [],
   "source": [
    "np.mean(rmse(xgb,train_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d6d00c64286220dcdbde61f0ee7d47b52865fc29"
   },
   "outputs": [],
   "source": [
    "lgbm_params = {'bagging_fraction': 0.1, 'feature_fraction': 0.1, 'lambda': 0.1, 'learning_rate': 0.0325, 'max_bin': 60, 'metric': 'mse', 'n_estimators': 750, 'num_leaves': 8, 'objective': 'regression', 'sub_feature': 0.5, 'verbose': 0}\n",
    "lgbm = lgb.LGBMRegressor(**lgbm_params)\n",
    "lgbm.fit(train_x,train_y)\n",
    "lgbm_preds = lgbm.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0111bdb6d887f30803b43f97c7e06135800e1f9"
   },
   "outputs": [],
   "source": [
    "lgbm_params = {'bagging_fraction': 0.1, 'feature_fraction': 0.1, 'lambda': 0.1, 'learning_rate': 0.0325, 'max_bin': 60, 'metric': 'mse', 'n_estimators': 750, 'num_leaves': 8, 'objective': 'regression', 'sub_feature': 0.5, 'verbose': 0}\n",
    "lgbm2 = lgb.LGBMRegressor(**lgbm_params)\n",
    "np.mean(rmse(lgbm2,train_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ad2d17dd424eff74bf0341bf316bae4e7ffd90dc"
   },
   "outputs": [],
   "source": [
    "submit_frame = pd.DataFrame()\n",
    "submit_frame['Id'] = testID\n",
    "submit_frame['SalePrice'] = invPrice(xgb_preds)\n",
    "submit_frame.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d68c03d5afea3c9b9db194b8b02530d2360c7ea7"
   },
   "source": [
    "# Towards the future!\n",
    "\n",
    "This performance with a single model is pretty good, but we can do better. To climb the leader boards, it will be necessary to implement an ensemble model.   \n",
    "As a point of good practice, we should also have withheld a train-test split to test our models on in addition to using cross validation. To some extent this is done when you submit for ranking but it can't be done rapidly or in a manner that is conducive to good study of the model.\n",
    "\n",
    "For now, this will suffice until I feel like looking at this data set again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c7216026097809d71c6c2416532a661e5405499c"
   },
   "source": [
    "\n",
    "# Parameter notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ccd1437bd4c322eb59ed14399d3141c7a6752337"
   },
   "source": [
    "## XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cdbc2cb1ad2d0fb8e5740d62c877189d62a822a7"
   },
   "outputs": [],
   "source": [
    "xgb_params1 = {\n",
    "         \"learning_rate\": np.linspace(0.04,0.06,3),\n",
    "         \"max_depth\": [3,5,8],\n",
    "         \"n_estimators\": [2500],\n",
    "         \"colsample_bytree\": np.linspace(0.4,0.6,3),\n",
    "         \"gamma\": np.linspace(0.03,0.05,3),\n",
    "         \"silent\": [1],\n",
    "         \"min_child_weight\": [1],\n",
    "         \"reg_alpha\": [0.5],\n",
    "         \"reg_lambda\": [0.8],\n",
    "         \"subsample\": [0.5],\n",
    "         \"nthread\": [1]\n",
    "         }\n",
    "         \n",
    " # 0.9088979396366262\n",
    "#{'colsample_bytree': 0.4, 'gamma': 0.05, 'learning_rate': 0.04, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 2500, 'nthread': 1, 'reg_alpha': 0.5, 'reg_lambda': 0.8, 'silent': 1, 'subsample': 0.5}\n",
    "\n",
    "xgb_params2 = {\n",
    "         \"learning_rate\": np.linspace(0.025,0.04,3),\n",
    "         \"max_depth\": [3,4],\n",
    "         \"n_estimators\": [2000,3000],\n",
    "         \"colsample_bytree\": np.linspace(0.2,0.4,3),\n",
    "         \"gamma\": np.linspace(0.05,0.075,3),\n",
    "         \"silent\": [1],\n",
    "         \"min_child_weight\": [1],\n",
    "         \"reg_alpha\": [0.5],\n",
    "         \"reg_lambda\": [0.8],\n",
    "         \"subsample\": [0.5],\n",
    "         \"nthread\": [1]\n",
    "         }\n",
    "         \n",
    "# 0.9117986931609089\n",
    "# {'colsample_bytree': 0.2, 'gamma': 0.05, 'learning_rate': 0.0325, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 2000, 'nthread': 1, 'reg_alpha': 0.5, 'reg_lambda': 0.8, 'silent': 1, 'subsample': 0.5}\n",
    "\n",
    "xgb_params3 = {\n",
    "         \"learning_rate\": np.linspace(0.0275,0.035,4),\n",
    "         \"max_depth\": [3],\n",
    "         \"n_estimators\": [2000,2500],\n",
    "         \"colsample_bytree\": np.linspace(0.05,0.25,4),\n",
    "         \"gamma\": np.linspace(0.01,0.075,3),\n",
    "         \"silent\": [1],\n",
    "         \"min_child_weight\": [1],\n",
    "         \"reg_alpha\": [0.5],\n",
    "         \"reg_lambda\": [0.8],\n",
    "         \"subsample\": [0.5],\n",
    "         \"nthread\": [1]\n",
    "         }\n",
    "\n",
    "# 0.9149185558702502\n",
    "# {'colsample_bytree': 0.05, 'gamma': 0.01, 'learning_rate': 0.0325, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 2000, 'nthread': 1, 'reg_alpha': 0.5, 'reg_lambda': 0.8, 'silent': 1, 'subsample': 0.5}\n",
    "\n",
    "xgb_params4 = {\n",
    "         \"learning_rate\": [0.0325],\n",
    "         \"max_depth\": [3],\n",
    "         \"n_estimators\": [1750,2000],\n",
    "         \"colsample_bytree\": np.linspace(0.01,0.1,4),\n",
    "         \"gamma\": [0.01],\n",
    "         \"silent\": [1],\n",
    "         \"min_child_weight\": [1],\n",
    "         \"reg_alpha\": np.linspace(0.25,0.75,5),\n",
    "         \"reg_lambda\": np.linspace(0.7,0.9,3),\n",
    "         \"subsample\": [0.5],\n",
    "         \"nthread\": [1]\n",
    "         }\n",
    "\n",
    "# 0.9137795024094789\n",
    "# {'colsample_bytree': 0.07, 'gamma': 0.01, 'learning_rate': 0.0325, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 2000, 'nthread': 1, 'reg_alpha': 0.375, 'reg_lambda': 0.7, 'silent': 1, 'subsample': 0.5}\n",
    "\n",
    "xgb_params4 = {\n",
    "         \"learning_rate\": [0.0325],\n",
    "         \"max_depth\": [3],\n",
    "         \"n_estimators\": [1900,2100],\n",
    "         \"colsample_bytree\": np.linspace(0.65,0.75,3),              # There was a mistake on the order of magnitude for these values in this run\n",
    "         \"gamma\": [0.01],\n",
    "         \"silent\": [1],\n",
    "         \"min_child_weight\": [1],\n",
    "         \"reg_alpha\": np.linspace(0.3,0.5,5),\n",
    "         \"reg_lambda\": np.linspace(0.6,0.75,4),\n",
    "         \"subsample\": np.linspace(0.4,0.5,3),\n",
    "         \"nthread\": [1]\n",
    "         }\n",
    "\n",
    "# 0.9106672141801405\n",
    "# {'colsample_bytree': 0.65, 'gamma': 0.01, 'learning_rate': 0.0325, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 1900, 'nthread': 1, 'reg_alpha': 0.3, 'reg_lambda': 0.6, 'silent': 1, 'subsample': 0.5}\n",
    "\n",
    "xgb_params5 = {\n",
    "         \"learning_rate\": np.linspace(0.032,0.033,3),\n",
    "         \"max_depth\": [3],\n",
    "         \"n_estimators\": [2000],\n",
    "         \"colsample_bytree\": np.linspace(0.065,0.075,3),\n",
    "         \"gamma\": [0.01],\n",
    "         \"silent\": [1],\n",
    "         \"min_child_weight\": [1],\n",
    "         \"reg_alpha\": np.linspace(0.2,0.4,3),\n",
    "         \"reg_lambda\": np.linspace(0.55,0.65,3),\n",
    "         \"subsample\": [0.5],\n",
    "         \"nthread\": [1]\n",
    "         }\n",
    "# 0.9118781247040025\n",
    "# {'colsample_bytree': 0.075, 'gamma': 0.01, 'learning_rate': 0.0325, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 2000, 'nthread': 1, 'reg_alpha': 0.2, 'reg_lambda': 0.65, 'silent': 1, 'subsample': 0.5}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "68e8be14a70fa5f40cf197afba0b838f852bf73c"
   },
   "source": [
    "## Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "35ec0c53417d38eeac52eb44f6995b5fb4c447d1"
   },
   "outputs": [],
   "source": [
    "lgbm_params1 = {'objective' : ['regression'],\n",
    "               'metric': ['mse'],\n",
    "               'num_leaves' : [3,5,8],\n",
    "               'learning_rate' : np.linspace(0.01,0.1,5),\n",
    "               'lambda': np.linspace(0.1,0.9,5),\n",
    "               'max_bin': [40,50,60],\n",
    "               'bagging_fraction' : np.linspace(0.1,0.9,2),\n",
    "               'feature_fraction' : np.linspace(0.1,0.9,2),\n",
    "               'n_estimators': [750,1000],\n",
    "               'sub_feature' : [0.5],\n",
    "               'verbose': [0]\n",
    "              }\n",
    "\n",
    "# 0.9012716484076563\n",
    "# {'bagging_fraction': 0.1, 'feature_fraction': 0.1, 'lambda': 0.1, 'learning_rate': 0.0325, 'max_bin': 60, 'metric': 'mse', 'n_estimators': 750, 'num_leaves': 8, 'objective': 'regression', 'sub_feature': 0.5, 'verbose': 0}\n",
    "\n",
    "lgbm_params2 = {'objective' : ['regression'],\n",
    "               'metric': ['mse'],\n",
    "               'num_leaves' : [10,50,100],\n",
    "               'learning_rate' : [0.0325],\n",
    "               'lambda': np.linspace(0.1,0.5,3),\n",
    "               'max_bin': [60,100],\n",
    "               'bagging_fraction' : np.linspace(0.05,0.15,3),\n",
    "               'feature_fraction' : np.linspace(0.05,0.15,3),\n",
    "               'n_estimators': [750],\n",
    "               'sub_feature' : [0.5],\n",
    "               'verbose': [0]\n",
    "              }\n",
    "\n",
    "# 0.9007668429828806\n",
    "# {'bagging_fraction': 0.05, 'feature_fraction': 0.1, 'lambda': 0.1, 'learning_rate': 0.0325, 'max_bin': 60, 'metric': 'mse', 'n_estimators': 750, 'num_leaves': 10, 'objective': 'regression', 'sub_feature': 0.5, 'verbose': 0}\n",
    "\n",
    "lgbm_params3 = {'objective' : ['regression'],\n",
    "               'metric': ['mse'],\n",
    "               'num_leaves' : [8,10,15],\n",
    "               'learning_rate' : [0.0325],\n",
    "               'lambda': np.linspace(0.01,0.1,5),\n",
    "               'max_bin': [60,100],\n",
    "               'bagging_fraction' : [0.05],\n",
    "               'feature_fraction' : [0.1],\n",
    "               'num_rounds' : [50,100,500],\n",
    "               'n_estimators': [750],\n",
    "               'sub_feature' : [0.5],\n",
    "               'verbose': [0]\n",
    "              }\n",
    "\n",
    "# 0.9007548589509198\n",
    "# {'bagging_fraction': 0.05, 'feature_fraction': 0.1, 'lambda': 0.01, 'learning_rate': 0.0325, 'max_bin': 60, 'metric': 'mse', 'n_estimators': 750, 'num_leaves': 8, 'num_rounds': 500, 'objective': 'regression', 'sub_feature': 0.5, 'verbose': 0}\n",
    "\n",
    "lgbm_params4 = {'objective' : ['regression'],\n",
    "               'metric': ['mse'],\n",
    "               'num_leaves' : [8],\n",
    "               'learning_rate' : np.linspace(0.030,0.035,5),\n",
    "               'lambda': np.linspace(0.005,0.01,5),\n",
    "               'max_bin': [60,100],\n",
    "               'bagging_fraction' : [0.05],\n",
    "               'feature_fraction' : [0.1],\n",
    "               'num_rounds' : [500],\n",
    "               'n_estimators': [750],\n",
    "               'sub_feature' : [0.5],\n",
    "               'verbose': [0]\n",
    "              }\n",
    "\n",
    "# 0.9008893309397983\n",
    "# {'bagging_fraction': 0.05, 'feature_fraction': 0.1, 'lambda': 0.005, 'learning_rate': 0.03375, 'max_bin': 60, 'metric': 'mse', 'n_estimators': 750, 'num_leaves': 8, 'num_rounds': 500, 'objective': 'regression', 'sub_feature': 0.5, 'verbose': 0}\n",
    "\n",
    "lgbm_params5 = {'objective' : ['regression'],\n",
    "               'metric': ['mse'],\n",
    "               'num_leaves' : [8],\n",
    "               'learning_rate' : np.linspace(0.033,0.035,5),\n",
    "               'lambda': np.linspace(0.00,0.01,5),\n",
    "               'max_bin': [60,100],\n",
    "               'bagging_fraction' : [0.05],\n",
    "               'feature_fraction' : [0.1],\n",
    "               'num_rounds' : [500],\n",
    "               'n_estimators': [750],\n",
    "               'sub_feature' : [0.5],\n",
    "               'verbose': [0]\n",
    "              }\n",
    "# 0.9009788735426308\n",
    "# {'bagging_fraction': 0.05, 'feature_fraction': 0.1, 'lambda': 0.0, 'learning_rate': 0.034, 'max_bin': 60, 'metric': 'mse', 'n_estimators': 750, 'num_leaves': 8, 'num_rounds': 500, 'objective': 'regression', 'sub_feature': 0.5, 'verbose': 0}\n",
    "\n",
    "\n",
    "lgbm_params6 = {'objective' : ['regression'],\n",
    "               'metric': ['mse'],\n",
    "               'num_leaves' : [8,100],\n",
    "               'learning_rate' : [0.001,0.01,0.03,0.1],\n",
    "               'lambda': np.linspace(0.02,0.9,5),\n",
    "               'max_bin': [20,60,100],\n",
    "               'bagging_fraction' : np.linspace(0.01,0.9,4),\n",
    "               'feature_fraction' : np.linspace(0.01,0.5,4),\n",
    "               'num_rounds' : [500],\n",
    "               'n_estimators': [750],\n",
    "               'sub_feature' : [0.5],\n",
    "               'verbose': [0]\n",
    "              }\n",
    "\n",
    "#  0.29463939175444454\n",
    "# -0.09921780759203461\n",
    "# {'bagging_fraction': 0.01, 'feature_fraction': 0.17333333333333334, 'lambda': 0.02, 'learning_rate': 0.03, 'max_bin': 100, 'metric': 'mse', 'n_estimators': 750, 'num_leaves': 8, 'num_rounds': 500, 'objective': 'regression', 'sub_feature': 0.5, 'verbose': 0}\n",
    "\n",
    "lgbm_params7 = {'objective' : ['regression'],\n",
    "               'metric': ['mse'],\n",
    "               'num_leaves' : [5,8,12],\n",
    "               'learning_rate' : np.linspace(0.03,0.035,3),\n",
    "               'lambda': np.linspace(0.01,0.03,3),\n",
    "               'max_bin': [100],\n",
    "               'bagging_fraction' : [0.01],\n",
    "               'feature_fraction' : [0.173],\n",
    "               'num_rounds' : [500],\n",
    "               'n_estimators': [750],\n",
    "               'sub_feature' : [0.5],\n",
    "               'verbose': [0]\n",
    "              }\n",
    "\n",
    "#  0.2952754856025843\n",
    "# -0.09286440091369501\n",
    "# {'bagging_fraction': 0.01, 'feature_fraction': 0.173, 'lambda': 0.01, 'learning_rate': 0.0325, 'max_bin': 100, 'metric': 'mse', 'n_estimators': 750, 'num_leaves': 12, 'num_rounds': 500, 'objective': 'regression', 'sub_feature': 0.5, 'verbose': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c7ae0fd3dfa5b08216533810209f96e04fcaf120"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
